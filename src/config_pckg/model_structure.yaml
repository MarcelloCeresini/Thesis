# name: EncodeProcessDecode_Baseline
name: EPD_with_sampling
PINN: True
add_self_loops: False
update_edges: True
edges_channels: 4
use_positional_features: True

positional_encoder:
  type: sequential_model
  layers: 
    # - {type: layer, name: Linear, out_features: 32}
    # - {type: layer, name: Softplus}
    # - {type: layer, name: Linear, out_features: 64}
    # - {type: layer, name: Softplus}

encoder:
  type: sequential_model
  layers: 
    # PUT A HIGHER NUMBER HERE --> at least 32 because it enters with ~160 channels
    - {type: layer, name: Linear, out_features: 64}
    - {type: layer, name: SiLU}
    # Last layer to get out_channels = message_passer["out_channels"] is automatic

edge_encoder:
  type: sequential_model
  layers: 
    # - {type: layer, name: Linear, out_features: 8} 
    # - {type: layer, name: Softplus}

message_passer:
  type: repeated_shared_layer
  repeats_training: 10
  name: Simple_MLPConv # [NNConv, GCNConv, MLPConv, Simple_MLPConv_edges]
  out_channels: 16 # also in_channels are the same, to be able to call multiple times
  attention: False
  k_heads: 4
  channels_per_head: 16
  att_channels: 16
  edge_in_channels: 4
  add_global_info: True
  add_BC_info: True
  skip: True
  standard_activation: SiLU
  aggr: mean
  nn: # takes as input edge_features (and does NOT update them, always gets edge features)
    type: sequential_model
    layers: 
      # - {type: layer, name: Linear, out_features: 32}
      # - {type: layer, name: SiLU}
      # Last (Linear) layer for internal_mlp is always added autonomously
  nn_update:
    type: sequential_model
    layers: 
      # - {type: layer, name: Linear, out_features: 32}
      # - {type: layer, name: SiLU}
  nn_edges:
    type: sequential_model
    layers: 
      # - {type: layer, name: Linear, out_features: 8}
      # - {type: layer, name: SiLU}

new_edges_mlp:
  add_global_info: True
  add_BC_info: True
  standard_activation: SiLU
  msg_mlp:
    type: sequential_model
    layers: 
  update_mlp:
    type: sequential_model
    layers:

decoder:
  type: sequential_model
  act_for_max_norm_feat: Tanhshrink # Sigmoid, ReLU, Softplus, Tanhshrink, SiLU
  layers: 
    # - {type: layer, name: Linear, out_features: 128}
    # - {type: layer, name: Softplus}
    # - {type: layer, name: Linear, out_features: 32}
    # - {type: layer, name: Softplus}
    # Last (Linear) layer for regression is always added by model autonomously

