model:
  n_message_passing_layers: 3
training:
  lr: 0.001
  weight_decay: 0.001
  n_epochs: 60
  patience_reduce_lr: 10
  batch_size: 24
val:
  batch_size: 8
  n_epochs_val: 1

